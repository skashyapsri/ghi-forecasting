{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skashyapsri/ghi-forecasting/blob/main/AST_Latest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Laa-BhLfCI9b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def process_nasa_data(data):\n",
        "    ghi_data = []\n",
        "    time_data = []\n",
        "\n",
        "    ghi_dict = data['properties']['parameter']['ALLSKY_SFC_SW_DWN']\n",
        "    for timestamp, value in ghi_dict.items():\n",
        "        if len(timestamp) == 10:  # Full timestamp format YYYYMMDDHH\n",
        "            ghi_data.append(float(value))\n",
        "            time_data.append(datetime.strptime(timestamp, '%Y%m%d%H'))\n",
        "\n",
        "    return np.array(ghi_data)\n",
        "\n",
        "\n",
        "# NASA POWER API data fetcher\n",
        "def fetch_power_data(lat, lon, start_date, end_date):\n",
        "    base_url = \"https://power.larc.nasa.gov/api/temporal/hourly/point\"\n",
        "    params = {\n",
        "        \"parameters\": \"ALLSKY_SFC_SW_DWN\",  # GHI parameter\n",
        "        \"community\": \"RE\",\n",
        "        \"longitude\": lon,\n",
        "        \"latitude\": lat,\n",
        "        \"start\": start_date,\n",
        "        \"end\": end_date,\n",
        "        \"format\": \"JSON\"\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "    return response.json()\n",
        "\n",
        "# Data preprocessing\n",
        "class DataPreprocessor:\n",
        "    def __init__(self, window_size=24, prediction_hours=1):\n",
        "        self.window_size = window_size\n",
        "        self.prediction_hours = prediction_hours\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def create_sequences(self, data):\n",
        "        X, y = [], []\n",
        "        data = data.reshape(-1, 1)\n",
        "        for i in range(len(data) - self.window_size - self.prediction_hours):\n",
        "            X.append(data[i:(i + self.window_size)])\n",
        "            y.append(data[i + self.window_size:i + self.window_size + self.prediction_hours])\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    def normalize_data(self, data):\n",
        "        data = data.reshape(-1, 1)\n",
        "        return self.scaler.fit_transform(data)\n",
        "\n",
        "# AST Model Implementation\n",
        "class AST(keras.Model):\n",
        "    def __init__(self, seq_len, pred_len, d_model=256, n_heads=4, n_layers=3):\n",
        "        super(AST, self).__init__()\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "\n",
        "        # Embedding layers\n",
        "        self.input_proj = layers.Dense(d_model)\n",
        "        self.pos_encoding = self._positional_encoding(seq_len, d_model)\n",
        "\n",
        "        # Sparse Transformer Encoder\n",
        "        self.encoder_layers = [\n",
        "            SparseTransformerLayer(d_model, n_heads)\n",
        "            for _ in range(n_layers)\n",
        "        ]\n",
        "\n",
        "        # Output projection\n",
        "        self.output_proj = layers.Dense(1)\n",
        "\n",
        "    def _positional_encoding(self, seq_len, d_model):\n",
        "        positions = np.arange(seq_len)[:, np.newaxis]\n",
        "        angles = np.arange(d_model)[np.newaxis, :] / d_model\n",
        "        angles = positions * angles\n",
        "\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Input projection and positional encoding\n",
        "        x = self.input_proj(inputs)\n",
        "        x += self.pos_encoding\n",
        "\n",
        "        # Encoder layers\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            x = encoder_layer(x)\n",
        "\n",
        "        # Output projection\n",
        "        output = self.output_proj(x)\n",
        "        return output\n",
        "\n",
        "class SparseTransformerLayer(layers.Layer):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super(SparseTransformerLayer, self).__init__()\n",
        "\n",
        "        self.mha = layers.MultiHeadAttention(n_heads, d_model)\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(d_model * 4, activation='relu'),\n",
        "            layers.Dense(d_model)\n",
        "        ])\n",
        "\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = layers.Dropout(0.1)\n",
        "        self.dropout2 = layers.Dropout(0.1)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Multi-head attention with sparse attention mask\n",
        "        attn_output = self.mha(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        # Feed forward network\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Training\n",
        "def train_model(model, train_data, val_data, epochs=100):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(x, y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(x, training=True)\n",
        "            loss = loss_fn(y, predictions)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    @tf.function\n",
        "    def val_step(x, y):\n",
        "        predictions = model(x, training=False)\n",
        "        loss = loss_fn(y, predictions)\n",
        "        return loss\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Track loss for each epoch\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for x_batch, y_batch in train_data:\n",
        "            train_loss = train_step(x_batch, y_batch)\n",
        "            train_losses.append(float(train_loss))\n",
        "\n",
        "        for x_batch, y_batch in val_data:\n",
        "            val_loss = val_step(x_batch, y_batch)\n",
        "            val_losses.append(float(val_loss))\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, \"\n",
        "              f\"Train Loss: {np.mean(train_losses):.4f}, \"\n",
        "              f\"Val Loss: {np.mean(val_losses):.4f}\")\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Fetch data\n",
        "    lat, lon = 40.7128, -74.0060  # New York City coordinates\n",
        "    data = fetch_power_data(lat, lon, \"20230101\", \"20231231\")\n",
        "    ghi_data = process_nasa_data(data)\n",
        "\n",
        "    # Preprocess\n",
        "    preprocessor = DataPreprocessor(window_size=24, prediction_hours=1)\n",
        "    normalized_data = preprocessor.normalize_data(ghi_data)\n",
        "    X, y = preprocessor.create_sequences(normalized_data)\n",
        "\n",
        "    # Split data\n",
        "    split = int(0.8 * len(X))\n",
        "    X_train, X_val = X[:split], X[split:]\n",
        "    y_train, y_val = y[:split], y[split:]\n",
        "\n",
        "    # Create data loaders\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(32)\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = AST(seq_len=24, pred_len=1)\n",
        "    train_model(model, train_dataset, val_dataset)\n",
        "\n",
        "    # Save model\n",
        "    model.save('ast_ghi_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "TORCH_USE_CUDA_DSA=1\n",
        "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
        "os.environ[\"PYTORCH_USE_CUDA_DSA\"] = \"1\"\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "\n",
        "# NASA POWER API data fetcher\n",
        "def fetch_power_data(lat, lon, start_date, end_date):\n",
        "    base_url = \"https://power.larc.nasa.gov/api/temporal/hourly/point\"\n",
        "    params = {\n",
        "        \"parameters\": \"ALLSKY_SFC_SW_DWN\",  # GHI parameter\n",
        "        \"community\": \"RE\",\n",
        "        \"longitude\": lon,\n",
        "        \"latitude\": lat,\n",
        "        \"start\": start_date,\n",
        "        \"end\": end_date,\n",
        "        \"format\": \"JSON\"\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "    return response.json()\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data, seq_length, pred_length):\n",
        "        self.data = torch.FloatTensor(data)\n",
        "        self.seq_length = seq_length\n",
        "        self.pred_length = pred_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_length - self.pred_length + 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx + self.seq_length].unsqueeze(-1)  # Add feature dimension\n",
        "        y = self.data[idx + self.seq_length:idx + self.seq_length + self.pred_length].unsqueeze(-1)\n",
        "        return x, y\n",
        "\n",
        "class AlphaEntmax(nn.Module):\n",
        "    def __init__(self, alpha=1.5, dim=-1):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.entmax(x, self.alpha, dim=self.dim)\n",
        "\n",
        "    def entmax(self, x, alpha, dim=-1):\n",
        "        if alpha == 1:\n",
        "            return torch.softmax(x, dim=dim)\n",
        "\n",
        "        x_shifted = x - x.max(dim=dim, keepdim=True)[0]\n",
        "        tau = self._find_tau(x_shifted, alpha, dim)\n",
        "        p = torch.clamp(((alpha - 1) * x_shifted - tau) / alpha, min=0) ** (1 / (alpha - 1))\n",
        "        return p\n",
        "\n",
        "    def _find_tau(self, x, alpha, dim):\n",
        "        n = x.shape[dim]\n",
        "        tau_lower = x.min(dim=dim, keepdim=True)[0] * (alpha - 1)\n",
        "        tau_upper = x.max(dim=dim, keepdim=True)[0] * (alpha - 1)\n",
        "\n",
        "        for _ in range(20):\n",
        "            tau = (tau_lower + tau_upper) / 2\n",
        "            p = torch.clamp(((alpha - 1) * x - tau) / alpha, min=0) ** (1 / (alpha - 1))\n",
        "            sum_p = p.sum(dim=dim, keepdim=True)\n",
        "            too_high = (sum_p > 1)\n",
        "            tau_lower = torch.where(too_high, tau, tau_lower)\n",
        "            tau_upper = torch.where(too_high, tau_upper, tau)\n",
        "\n",
        "        return tau\n",
        "\n",
        "class SparseMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1, alpha=1.5):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.out_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.alpha_entmax = AlphaEntmax(alpha=alpha)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        # Linear transformations\n",
        "        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        if mask is not None:\n",
        "            # Ensure mask matches scores dimensions\n",
        "            if mask.dim() == 3:\n",
        "                mask = mask.unsqueeze(1)\n",
        "            elif mask.dim() == 2:\n",
        "                mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "            # Ensure mask size matches scores size\n",
        "            scores_size = scores.size()\n",
        "            mask = mask[:, :, :scores_size[2], :scores_size[3]]\n",
        "\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        # Apply sparse attention\n",
        "        attn = self.alpha_entmax(scores)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # Compute weighted sum\n",
        "        output = torch.matmul(attn, v)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        return self.out_linear(output)\n",
        "\n",
        "def create_mask(seq_len):\n",
        "    \"\"\"\n",
        "    Creates a causal mask for the decoder self-attention.\n",
        "    Args:\n",
        "        seq_len: Length of the sequence\n",
        "    Returns:\n",
        "        mask: Binary mask where 1 indicates attention is allowed\n",
        "    \"\"\"\n",
        "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
        "    return ~mask  # Invert to get attention mask where 1 means attend, 0 means don't attend\n",
        "\n",
        "def create_src_mask(src):\n",
        "    \"\"\"\n",
        "    Creates a mask for the encoder self-attention.\n",
        "    Args:\n",
        "        src: Source sequence tensor of shape [batch_size, seq_len, ...]\n",
        "    Returns:\n",
        "        mask: Binary mask where 1 indicates attention is allowed\n",
        "    \"\"\"\n",
        "    batch_size, seq_len = src.size(0), src.size(1)\n",
        "    return torch.ones((batch_size, seq_len, seq_len)).to(src.device)\n",
        "\n",
        "def create_tgt_mask(tgt):\n",
        "    \"\"\"\n",
        "    Creates a causal mask for the decoder self-attention.\n",
        "    Args:\n",
        "        tgt: Target sequence tensor of shape [batch_size, seq_len, ...]\n",
        "    Returns:\n",
        "        mask: Binary mask where 1 indicates attention is allowed\n",
        "    \"\"\"\n",
        "    batch_size, seq_len = tgt.size(0), tgt.size(1)\n",
        "    mask = create_mask(seq_len)\n",
        "    return mask.unsqueeze(0).expand(batch_size, -1, -1).to(tgt.device)\n",
        "\n",
        "def train_ast(data, seq_length=168, pred_length=24, batch_size=32, epochs=100,\n",
        "              d_model=256, num_heads=8, num_layers=3, dropout=0.1):\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = TimeSeriesDataset(data, seq_length, pred_length)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize models\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = AST(input_dim=1, output_dim=1, d_model=d_model,\n",
        "               num_heads=num_heads, num_layers=num_layers, dropout=dropout).to(device)\n",
        "    discriminator = Discriminator(pred_length).to(device)\n",
        "\n",
        "    # Initialize optimizers\n",
        "    g_optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0001)\n",
        "\n",
        "    # Loss functions\n",
        "    mse_loss = nn.MSELoss()\n",
        "    bce_loss = nn.BCELoss()\n",
        "\n",
        "    # Initialize training history\n",
        "    training_history = {\n",
        "        'g_loss': [],\n",
        "        'd_loss': [],\n",
        "        'g_loss_avg': [],\n",
        "        'd_loss_avg': []\n",
        "    }\n",
        "\n",
        "    print(\"Training on device:\", device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        discriminator.train()\n",
        "        total_g_loss = 0\n",
        "        total_d_loss = 0\n",
        "\n",
        "        for batch_idx, (x, y) in enumerate(dataloader):\n",
        "            batch_size = x.size(0)\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # Train discriminator\n",
        "            d_optimizer.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                fake_seq = model(x, y)\n",
        "            real_labels = torch.ones(batch_size, 1).to(device)\n",
        "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "            d_real = discriminator(y)\n",
        "            d_fake = discriminator(fake_seq.detach())\n",
        "\n",
        "            d_real_loss = bce_loss(d_real, real_labels)\n",
        "            d_fake_loss = bce_loss(d_fake, fake_labels)\n",
        "            d_loss = d_real_loss + d_fake_loss\n",
        "\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "            # Train generator\n",
        "            g_optimizer.zero_grad()\n",
        "            fake_seq = model(x, y)\n",
        "            d_fake = discriminator(fake_seq)\n",
        "\n",
        "            mse = mse_loss(fake_seq, y)\n",
        "            adversarial_loss = bce_loss(d_fake, real_labels)\n",
        "            g_loss = mse + 0.1 * adversarial_loss\n",
        "\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "            # Record batch losses\n",
        "            training_history['g_loss'].append(g_loss.item())\n",
        "            training_history['d_loss'].append(d_loss.item())\n",
        "\n",
        "            total_g_loss += g_loss.item()\n",
        "            total_d_loss += d_loss.item()\n",
        "\n",
        "        # Calculate and store average losses for the epoch\n",
        "        avg_g_loss = total_g_loss / len(dataloader)\n",
        "        avg_d_loss = total_d_loss / len(dataloader)\n",
        "        training_history['g_loss_avg'].append(avg_g_loss)\n",
        "        training_history['d_loss_avg'].append(avg_d_loss)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], G Loss: {avg_g_loss:.4f}, D Loss: {avg_d_loss:.4f}')\n",
        "\n",
        "    # Attach training history to the model\n",
        "    model.training_history = training_history\n",
        "    discriminator.training_history = training_history\n",
        "\n",
        "    return model, discriminator\n",
        "\n",
        "\n",
        "class AST(nn.Module):\n",
        "    def __init__(self, input_dim=1, output_dim=1, d_model=256, num_heads=8, num_layers=3, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Input embedding\n",
        "        self.input_embedding = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.register_buffer('pos_encoding', self._create_positional_encoding(5000, d_model))\n",
        "\n",
        "        # Encoder and decoder layers with layer normalization\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            nn.TransformerDecoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        self.output_linear = nn.Linear(d_model, output_dim)\n",
        "        self.training_history = None\n",
        "\n",
        "    def _create_positional_encoding(self, max_seq_len, d_model):\n",
        "        pos_encoding = torch.zeros(max_seq_len, d_model)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        return pos_encoding\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Add batch dimension if not present\n",
        "        if src.dim() == 2:\n",
        "            src = src.unsqueeze(0)\n",
        "        if tgt.dim() == 2:\n",
        "            tgt = tgt.unsqueeze(0)\n",
        "\n",
        "        # Create masks\n",
        "        src_mask = None  # Allow attending to all source positions\n",
        "        tgt_mask = self._generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
        "\n",
        "        # Embedding and positional encoding\n",
        "        src = self.input_embedding(src)\n",
        "        tgt = self.input_embedding(tgt)\n",
        "\n",
        "        src = src + self.pos_encoding[:src.size(1)]\n",
        "        tgt = tgt + self.pos_encoding[:tgt.size(1)]\n",
        "\n",
        "        # Transpose for transformer input [seq_len, batch, features]\n",
        "        src = src.transpose(0, 1)\n",
        "        tgt = tgt.transpose(0, 1)\n",
        "\n",
        "        # Encoder\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            src = enc_layer(src, src_mask)\n",
        "\n",
        "        # Decoder\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            tgt = dec_layer(tgt, src, tgt_mask)\n",
        "\n",
        "        # Transpose back [batch, seq_len, features]\n",
        "        output = tgt.transpose(0, 1)\n",
        "\n",
        "        return self.output_linear(output)\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, seq_len, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(seq_len, hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim // 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1)  # Flatten the sequence\n",
        "        return self.model(x)\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = SparseMultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model * 4, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = SparseMultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.cross_attn = SparseMultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model * 4, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        cross_attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
        "\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "kv2AO6BUCJ-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import torch\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import os\n",
        "\n",
        "# City coordinates from your thesis\n",
        "CITIES = {\n",
        "    'Delhi': (28.6139, 77.2090),\n",
        "    'Mumbai': (19.0760, 72.8777),\n",
        "    'Chennai': (13.0827, 80.2707),\n",
        "    'Kolkata': (22.5726, 88.3639),\n",
        "    'Bangalore': (12.9716, 77.5946)\n",
        "}\n",
        "\n",
        "class MultiCityAnalysis:\n",
        "    def __init__(self, save_dir='results/'):\n",
        "        self.save_dir = save_dir\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        os.makedirs(f\"{save_dir}/plots\", exist_ok=True)\n",
        "        self.metrics = {}\n",
        "\n",
        "    def fetch_all_city_data(self):\n",
        "        \"\"\"Fetch and process data for all cities\"\"\"\n",
        "        city_data = {}\n",
        "        for city, (lat, lon) in CITIES.items():\n",
        "            print(f\"Fetching data for {city}...\")\n",
        "            data = fetch_power_data(lat, lon, \"20230101\", \"20231231\")\n",
        "            values = list(data['properties']['parameter']['ALLSKY_SFC_SW_DWN'].values())\n",
        "            values = [float(x) for x in values if isinstance(x, (int, float))]\n",
        "            normalized_data = (values - np.mean(values)) / np.std(values)\n",
        "            city_data[city] = {\n",
        "                'raw': values,\n",
        "                'normalized': normalized_data\n",
        "            }\n",
        "        return city_data\n",
        "\n",
        "    def train_models(self, city_data, seq_length=168, pred_length=24,\n",
        "                    batch_size=32, epochs=100):\n",
        "        \"\"\"Train models for each city\"\"\"\n",
        "        models = {}\n",
        "        training_histories = {}\n",
        "\n",
        "        for city, data in city_data.items():\n",
        "            print(f\"\\nTraining model for {city}...\")\n",
        "            model, discriminator = train_ast(\n",
        "                data['normalized'],\n",
        "                seq_length=seq_length,\n",
        "                pred_length=pred_length,\n",
        "                batch_size=batch_size,\n",
        "                epochs=epochs\n",
        "            )\n",
        "            models[city] = {\n",
        "                'generator': model,\n",
        "                'discriminator': discriminator\n",
        "            }\n",
        "\n",
        "            # Store training metrics\n",
        "            training_histories[city] = {\n",
        "                'generator_loss': model.training_history['g_loss'],\n",
        "                'discriminator_loss': model.training_history['d_loss']\n",
        "            }\n",
        "\n",
        "        return models, training_histories\n",
        "\n",
        "    def evaluate_models(self, models, city_data):\n",
        "        \"\"\"Evaluate model performance for each city\"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        for city, model_dict in models.items():\n",
        "            print(f\"\\nEvaluating model for {city}...\")\n",
        "            model = model_dict['generator']\n",
        "            data = city_data[city]['normalized']\n",
        "\n",
        "            # Generate predictions\n",
        "            test_seq = data[-192:]  # Last 8 days\n",
        "            input_seq = test_seq[:168]  # Use 7 days as input\n",
        "            true_values = test_seq[168:192]  # Next 24 hours as ground truth\n",
        "\n",
        "            with torch.no_grad():\n",
        "                predictions = model(torch.FloatTensor(input_seq).unsqueeze(0))\n",
        "                predictions = predictions.squeeze().numpy()\n",
        "\n",
        "            # Calculate metrics\n",
        "            metrics[city] = {\n",
        "                'MAE': mean_absolute_error(true_values, predictions),\n",
        "                'RMSE': np.sqrt(mean_squared_error(true_values, predictions)),\n",
        "                'MAPE': mean_absolute_percentage_error(true_values, predictions) * 100\n",
        "            }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_training_history(self, training_histories):\n",
        "        \"\"\"Plot training history for all cities\"\"\"\n",
        "        plt.figure(figsize=(15, 10))\n",
        "\n",
        "        for city, history in training_histories.items():\n",
        "            plt.plot(history['generator_loss'], label=f'{city} - Generator')\n",
        "\n",
        "        plt.title('Training History Across Cities')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Generator Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(f'{self.save_dir}/plots/training_history.png')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_prediction_comparison(self, models, city_data):\n",
        "        \"\"\"Plot prediction comparison for all cities\"\"\"\n",
        "        fig, axes = plt.subplots(3, 2, figsize=(20, 25))\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for idx, (city, model_dict) in enumerate(models.items()):\n",
        "            if idx >= len(axes):\n",
        "                break\n",
        "\n",
        "            model = model_dict['generator']\n",
        "            data = city_data[city]['normalized']\n",
        "            test_seq = data[-192:]\n",
        "            input_seq = test_seq[:168]\n",
        "            true_values = test_seq[168:192]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                predictions = model(torch.FloatTensor(input_seq).unsqueeze(0))\n",
        "                predictions = predictions.squeeze().numpy()\n",
        "\n",
        "            hours = np.arange(24)\n",
        "            axes[idx].plot(hours, true_values, 'b-', label='Actual')\n",
        "            axes[idx].plot(hours, predictions, 'r--', label='Predicted')\n",
        "            axes[idx].set_title(f'{city} - 24h Forecast')\n",
        "            axes[idx].set_xlabel('Hour')\n",
        "            axes[idx].set_ylabel('Normalized GHI')\n",
        "            axes[idx].legend()\n",
        "            axes[idx].grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.save_dir}/plots/prediction_comparison.png')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_metrics_comparison(self, metrics):\n",
        "        \"\"\"Plot performance metrics comparison\"\"\"\n",
        "        metrics_df = pd.DataFrame(metrics).T\n",
        "\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        metrics_df.plot(kind='bar')\n",
        "        plt.title('Performance Metrics Across Cities')\n",
        "        plt.xlabel('City')\n",
        "        plt.ylabel('Value')\n",
        "        plt.legend(title='Metric')\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.save_dir}/plots/metrics_comparison.png')\n",
        "        plt.close()\n",
        "\n",
        "    def save_results(self, metrics):\n",
        "        \"\"\"Save numerical results to CSV\"\"\"\n",
        "        metrics_df = pd.DataFrame(metrics).T\n",
        "        metrics_df.to_csv(f'{self.save_dir}/model_metrics.csv')"
      ],
      "metadata": {
        "id": "XfARwcmRTHhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GHIExploratoryAnalysis:\n",
        "    def __init__(self, save_dir='results/eda/'):\n",
        "        self.save_dir = save_dir\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        # plt.style.use('seaborn')\n",
        "\n",
        "    def run_complete_eda(self, city_data):\n",
        "        \"\"\"Run complete exploratory data analysis\"\"\"\n",
        "        print(\"\\nPerforming Exploratory Data Analysis...\")\n",
        "\n",
        "        # Validate data\n",
        "        for city, data in city_data.items():\n",
        "            print(f\"\\nValidating data for {city}:\")\n",
        "            print(f\"Number of raw data points: {len(data['raw'])}\")\n",
        "            print(f\"Number of normalized data points: {len(data['normalized'])}\")\n",
        "\n",
        "            # Check for missing or invalid values\n",
        "            raw_missing = np.sum(np.isnan(data['raw']))\n",
        "            norm_missing = np.sum(np.isnan(data['normalized']))\n",
        "            if raw_missing > 0 or norm_missing > 0:\n",
        "                print(f\"Warning: Found {raw_missing} missing raw values and {norm_missing} missing normalized values\")\n",
        "\n",
        "        # Create DataFrames for each city with datetime index\n",
        "        dfs = {}\n",
        "        for city, data in city_data.items():\n",
        "            # Get actual number of data points\n",
        "            n_points = len(data['raw'])\n",
        "            # Create date range matching the data length\n",
        "            dates = pd.date_range(start='2023-01-01', periods=n_points, freq='h')\n",
        "\n",
        "            # Create DataFrame with matching lengths\n",
        "            df = pd.DataFrame({\n",
        "                'GHI': data['raw'][:n_points],\n",
        "                'GHI_normalized': data['normalized'][:n_points]\n",
        "            }, index=dates[:n_points])\n",
        "            dfs[city] = df\n",
        "\n",
        "        # Generate all EDA plots\n",
        "        self.plot_daily_patterns(dfs)\n",
        "        self.plot_monthly_patterns(dfs)\n",
        "        self.plot_seasonal_patterns(dfs)\n",
        "        self.plot_city_distributions(dfs)\n",
        "        self.plot_correlation_heatmap(dfs)\n",
        "        self.plot_data_statistics(dfs)\n",
        "\n",
        "        # Print summary statistics\n",
        "        print(\"\\nSummary Statistics for Each City:\")\n",
        "        summary_stats = {}\n",
        "        for city, df in dfs.items():\n",
        "            print(f\"\\n{city}:\")\n",
        "            stats = df['GHI'].describe()\n",
        "            print(stats)\n",
        "            summary_stats[city] = stats\n",
        "\n",
        "            # Additional metrics\n",
        "            print(f\"\\nDaily max average: {df.groupby(df.index.date)['GHI'].max().mean():.2f}\")\n",
        "            print(f\"Daily duration of non-zero GHI: {(df['GHI'] > 0).groupby(df.index.date).sum().mean():.1f} hours\")\n",
        "\n",
        "        return dfs, summary_stats\n",
        "\n",
        "    def plot_daily_patterns(self, dfs):\n",
        "        \"\"\"Plot average daily GHI patterns for each city\"\"\"\n",
        "        plt.figure(figsize=(15, 8))\n",
        "\n",
        "        for city, df in dfs.items():\n",
        "            # Calculate average GHI for each hour\n",
        "            daily_pattern = df.groupby(df.index.hour)['GHI'].mean()\n",
        "            plt.plot(daily_pattern.index, daily_pattern.values, label=city, marker='o')\n",
        "\n",
        "        plt.title('Average Daily GHI Patterns Across Cities')\n",
        "        plt.xlabel('Hour of Day')\n",
        "        plt.ylabel('Average GHI (kW/m²)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(f'{self.save_dir}/daily_patterns.png')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_monthly_patterns(self, dfs):\n",
        "        \"\"\"Plot monthly GHI patterns\"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for idx, (city, df) in enumerate(dfs.items()):\n",
        "            if idx >= len(axes):\n",
        "                break\n",
        "\n",
        "            monthly_avg = df.groupby(df.index.month)['GHI'].agg(['mean', 'std'])\n",
        "            monthly_avg['mean'].plot(yerr=monthly_avg['std'],\n",
        "                                   capsize=5, marker='o', ax=axes[idx])\n",
        "            axes[idx].set_title(f'{city} - Monthly GHI Pattern')\n",
        "            axes[idx].set_xlabel('Month')\n",
        "            axes[idx].set_ylabel('GHI (kW/m²)')\n",
        "            axes[idx].grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.save_dir}/monthly_patterns.png')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_seasonal_patterns(self, dfs):\n",
        "        \"\"\"Plot seasonal patterns using boxplots\"\"\"\n",
        "        plt.figure(figsize=(15, 8))\n",
        "\n",
        "        seasonal_data = []\n",
        "        for city, df in dfs.items():\n",
        "            df['Season'] = pd.cut(df.index.month,\n",
        "                                bins=[0,3,6,9,12],\n",
        "                                labels=['Winter', 'Spring', 'Summer', 'Fall'])\n",
        "            for season in ['Winter', 'Spring', 'Summer', 'Fall']:\n",
        "                seasonal_data.append({\n",
        "                    'City': city,\n",
        "                    'Season': season,\n",
        "                    'GHI': df[df['Season'] == season]['GHI'].mean()\n",
        "                })\n",
        "\n",
        "        seasonal_df = pd.DataFrame(seasonal_data)\n",
        "        sns.boxplot(x='Season', y='GHI', hue='City', data=seasonal_df)\n",
        "\n",
        "        plt.title('Seasonal GHI Patterns Across Cities')\n",
        "        plt.ylabel('Average GHI (kW/m²)')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.save_dir}/seasonal_patterns.png')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_city_distributions(self, dfs):\n",
        "        \"\"\"Plot GHI distributions for each city\"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for idx, (city, df) in enumerate(dfs.items()):\n",
        "            if idx >= len(axes):\n",
        "                break\n",
        "\n",
        "            sns.histplot(data=df, x='GHI', kde=True, ax=axes[idx])\n",
        "            axes[idx].set_title(f'{city} - GHI Distribution')\n",
        "            axes[idx].set_xlabel('GHI (kW/m²)')\n",
        "\n",
        "            # Add distribution statistics\n",
        "            stats_text = f'Mean: {df[\"GHI\"].mean():.2f}\\n'\n",
        "            stats_text += f'Std: {df[\"GHI\"].std():.2f}\\n'\n",
        "            stats_text += f'Skew: {df[\"GHI\"].skew():.2f}'\n",
        "            axes[idx].text(0.95, 0.95, stats_text,\n",
        "                         transform=axes[idx].transAxes,\n",
        "                         verticalalignment='top',\n",
        "                         horizontalalignment='right',\n",
        "                         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.save_dir}/ghi_distributions.png')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_correlation_heatmap(self, dfs):\n",
        "        \"\"\"Plot correlation heatmap between cities\"\"\"\n",
        "        # Create DataFrame with all cities' GHI values\n",
        "        combined_df = pd.DataFrame({city: df['GHI'] for city, df in dfs.items()})\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(combined_df.corr(), annot=True, cmap='coolwarm', center=0)\n",
        "        plt.title('Inter-city GHI Correlation Heatmap')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.save_dir}/correlation_heatmap.png')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_data_statistics(self, dfs):\n",
        "        \"\"\"Plot key statistics for each city\"\"\"\n",
        "        stats = {}\n",
        "        for city, df in dfs.items():\n",
        "            stats[city] = {\n",
        "                'Mean': df['GHI'].mean(),\n",
        "                'Std': df['GHI'].std(),\n",
        "                'Max': df['GHI'].max(),\n",
        "                'Min': df['GHI'].min(),\n",
        "                'Skewness': df['GHI'].skew(),\n",
        "                'Kurtosis': df['GHI'].kurtosis()\n",
        "            }\n",
        "\n",
        "        stats_df = pd.DataFrame(stats).T\n",
        "\n",
        "        # Plot statistics\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for idx, column in enumerate(stats_df.columns):\n",
        "            if idx >= len(axes):\n",
        "                break\n",
        "\n",
        "            stats_df[column].plot(kind='bar', ax=axes[idx])\n",
        "            axes[idx].set_title(f'{column} by City')\n",
        "            axes[idx].set_ylabel(column)\n",
        "            plt.xticks(rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.save_dir}/data_statistics.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Save statistics to CSV\n",
        "        stats_df.to_csv(f'{self.save_dir}/data_statistics.csv')\n",
        "\n",
        "        return stats_df\n",
        "\n",
        "def extend_main_with_eda():\n",
        "    try:\n",
        "        # Initialize analyzers\n",
        "        analyzer = MultiCityAnalysis()\n",
        "        eda = GHIExploratoryAnalysis()\n",
        "\n",
        "        # Fetch data\n",
        "        city_data = analyzer.fetch_all_city_data()\n",
        "\n",
        "        # Perform EDA\n",
        "        print(\"\\nPerforming Exploratory Data Analysis...\")\n",
        "        dfs, summary_stats = eda.run_complete_eda(city_data)\n",
        "\n",
        "        # Additional analysis\n",
        "        print(\"\\nCross-city Analysis:\")\n",
        "        for city in city_data.keys():\n",
        "            daily_max = dfs[city].groupby(dfs[city].index.date)['GHI'].max()\n",
        "            daily_sum = dfs[city].groupby(dfs[city].index.date)['GHI'].sum()\n",
        "            print(f\"\\n{city}:\")\n",
        "            print(f\"Average daily maximum GHI: {daily_max.mean():.2f} kW/m²\")\n",
        "            print(f\"Average daily total GHI: {daily_sum.mean():.2f} kW/m²/day\")\n",
        "\n",
        "        # Train models with the processed data\n",
        "        models, training_histories = analyzer.train_models(city_data)\n",
        "\n",
        "        return city_data, dfs, summary_stats, models, training_histories\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during analysis: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "I4Dm9-K5TvKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GHIDataPreprocessor:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def preprocess_data(self, city_data):\n",
        "        \"\"\"Comprehensive data preprocessing pipeline\"\"\"\n",
        "        print(\"\\nPerforming data preprocessing...\")\n",
        "        processed_data = {}\n",
        "\n",
        "        for city, data in city_data.items():\n",
        "            print(f\"\\nProcessing {city} data:\")\n",
        "\n",
        "            # Convert to DataFrame with datetime index\n",
        "            dates = pd.date_range(start='2023-01-01', periods=len(data['raw']), freq='h')\n",
        "            df = pd.DataFrame({\n",
        "                'GHI': data['raw']\n",
        "            }, index=dates)\n",
        "\n",
        "            # 1. Handle missing values\n",
        "            df = self.handle_missing_values(df)\n",
        "\n",
        "            # 2. Remove physically impossible values\n",
        "            df = self.remove_impossible_values(df)\n",
        "\n",
        "            # 3. Apply temporal consistency check\n",
        "            df = self.ensure_temporal_consistency(df)\n",
        "\n",
        "            # 4. Normalize data\n",
        "            normalized_data = self.normalize_data(df['GHI'].values)\n",
        "\n",
        "            # Store processed data\n",
        "            processed_data[city] = {\n",
        "                'raw': df['GHI'].values,\n",
        "                'normalized': normalized_data,\n",
        "                'df': df\n",
        "            }\n",
        "\n",
        "            # Print preprocessing summary\n",
        "            self.print_preprocessing_summary(df, city)\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def handle_missing_values(self, df):\n",
        "        \"\"\"Handle missing values using appropriate interpolation methods\"\"\"\n",
        "        # Count initial missing values\n",
        "        initial_missing = df['GHI'].isna().sum()\n",
        "\n",
        "        if initial_missing > 0:\n",
        "            print(f\"Found {initial_missing} missing values\")\n",
        "\n",
        "            # For short gaps (≤3 hours), use linear interpolation\n",
        "            df['GHI'] = df['GHI'].interpolate(method='linear', limit=3)\n",
        "\n",
        "            # For medium gaps (3-6 hours), use spline interpolation\n",
        "            remaining_missing = df['GHI'].isna().sum()\n",
        "            if remaining_missing > 0:\n",
        "                df['GHI'] = df['GHI'].interpolate(method='spline', order=3, limit=6)\n",
        "\n",
        "            # For longer gaps, use pattern-based filling\n",
        "            remaining_missing = df['GHI'].isna().sum()\n",
        "            if remaining_missing > 0:\n",
        "                df = self.pattern_based_filling(df)\n",
        "\n",
        "            final_missing = df['GHI'].isna().sum()\n",
        "            print(f\"Filled {initial_missing - final_missing} missing values\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def remove_impossible_values(self, df):\n",
        "        \"\"\"Remove physically impossible GHI values\"\"\"\n",
        "        # GHI cannot be negative\n",
        "        impossible_low = (df['GHI'] < 0).sum()\n",
        "        if impossible_low > 0:\n",
        "            print(f\"Found {impossible_low} negative GHI values\")\n",
        "            df.loc[df['GHI'] < 0, 'GHI'] = 0\n",
        "\n",
        "        # GHI cannot exceed solar constant (~1361 W/m²)\n",
        "        impossible_high = (df['GHI'] > 1361).sum()\n",
        "        if impossible_high > 0:\n",
        "            print(f\"Found {impossible_high} impossibly high GHI values\")\n",
        "            df.loc[df['GHI'] > 1361, 'GHI'] = 1361\n",
        "\n",
        "        return df\n",
        "\n",
        "    def ensure_temporal_consistency(self, df):\n",
        "        \"\"\"Ensure temporal consistency in GHI values\"\"\"\n",
        "        # Calculate rate of change\n",
        "        df['GHI_diff'] = df['GHI'].diff()\n",
        "\n",
        "        # Flag sudden changes (more than 50% change in 1 hour)\n",
        "        sudden_changes = (abs(df['GHI_diff']) > 0.5 * df['GHI'].shift()).sum()\n",
        "        if sudden_changes > 0:\n",
        "            print(f\"Found {sudden_changes} sudden changes in GHI\")\n",
        "\n",
        "            # Smooth out extreme changes using rolling mean\n",
        "            mask = abs(df['GHI_diff']) > 0.5 * df['GHI'].shift()\n",
        "            df.loc[mask, 'GHI'] = df['GHI'].rolling(window=3, center=True).mean()\n",
        "\n",
        "        df.drop('GHI_diff', axis=1, inplace=True)\n",
        "        return df\n",
        "\n",
        "    def pattern_based_filling(self, df):\n",
        "        \"\"\"Fill missing values using pattern matching from similar days\"\"\"\n",
        "        for idx in df[df['GHI'].isna()].index:\n",
        "            # Find similar time points from other days\n",
        "            hour = idx.hour\n",
        "            similar_hours = df[df.index.hour == hour]['GHI'].dropna()\n",
        "\n",
        "            if len(similar_hours) > 0:\n",
        "                # Use median of similar hours\n",
        "                df.loc[idx, 'GHI'] = similar_hours.median()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def normalize_data(self, data):\n",
        "        \"\"\"Normalize GHI data using standardization\"\"\"\n",
        "        return (data - np.mean(data)) / np.std(data)\n",
        "\n",
        "    def print_preprocessing_summary(self, df, city):\n",
        "        \"\"\"Print summary of preprocessing results\"\"\"\n",
        "        print(f\"\\nPreprocessing summary for {city}:\")\n",
        "        print(f\"Total data points: {len(df)}\")\n",
        "        print(f\"Final missing values: {df['GHI'].isna().sum()}\")\n",
        "        print(f\"Data range: {df['GHI'].min():.2f} to {df['GHI'].max():.2f}\")\n",
        "        print(f\"Mean GHI: {df['GHI'].mean():.2f}\")\n",
        "        print(f\"Std GHI: {df['GHI'].std():.2f}\")\n",
        "\n",
        "def extend_main():\n",
        "    # Get results from main analysis\n",
        "    analyzer = MultiCityAnalysis()\n",
        "\n",
        "    # Fetch raw data\n",
        "    raw_city_data = analyzer.fetch_all_city_data()\n",
        "\n",
        "    # Preprocess data\n",
        "    preprocessor = GHIDataPreprocessor()\n",
        "    city_data = preprocessor.preprocess_data(raw_city_data)\n",
        "\n",
        "    # Perform EDA\n",
        "    eda_analyzer = GHIExploratoryAnalysis()\n",
        "    print(\"\\nPerforming Exploratory Data Analysis...\")\n",
        "    dfs, summary_stats = eda_analyzer.run_complete_eda(city_data)\n",
        "\n",
        "    # Additional analysis\n",
        "    print(\"\\nCross-city Analysis:\")\n",
        "    for city in city_data.keys():\n",
        "        daily_max = dfs[city].groupby(dfs[city].index.date)['GHI'].max()\n",
        "        daily_sum = dfs[city].groupby(dfs[city].index.date)['GHI'].sum()\n",
        "        print(f\"\\n{city}:\")\n",
        "        print(f\"Average daily maximum GHI: {daily_max.mean():.2f} kW/m²\")\n",
        "        print(f\"Average daily total GHI: {daily_sum.mean():.2f} kW/m²/day\")\n",
        "\n",
        "    # Train models\n",
        "    models, training_histories = analyzer.train_models(city_data)\n",
        "    metrics = analyzer.evaluate_models(models, city_data)\n",
        "\n",
        "    # Generate advanced visualizations\n",
        "    advanced_viz = AdvancedVisualizations()\n",
        "    advanced_viz.plot_attention_patterns(models, city_data)\n",
        "    advanced_viz.plot_temporal_stability(models, city_data)\n",
        "    advanced_viz.plot_error_distribution(models, city_data)\n",
        "\n",
        "    # Create performance summary\n",
        "    summary = advanced_viz.create_performance_summary(metrics, training_histories)\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "8AW0QK7dWV24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "\n",
        "class AdvancedVisualizations:\n",
        "    def __init__(self, save_dir='results/advanced_plots/'):\n",
        "        self.save_dir = save_dir\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    def plot_attention_patterns(self, models, city_data):\n",
        "        \"\"\"Visualize attention patterns for each city\"\"\"\n",
        "        fig, axes = plt.subplots(3, 2, figsize=(20, 25))\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for idx, (city, model_dict) in enumerate(models.items()):\n",
        "            if idx >= len(axes):\n",
        "                break\n",
        "\n",
        "            model = model_dict['generator']\n",
        "            # Extract attention weights from the model's last layer\n",
        "            with torch.no_grad():\n",
        "                attention_weights = model.get_attention_weights(\n",
        "                    torch.FloatTensor(city_data[city]['normalized'][-168:]).unsqueeze(0)\n",
        "                )\n",
        "\n",
        "            sns.heatmap(attention_weights[0].numpy(), ax=axes[idx],\n",
        "                       cmap='viridis', xticklabels=24, yticklabels=24)\n",
        "            axes[idx].set_title(f'{city} - Attention Patterns')\n",
        "            axes[idx].set_xlabel('Input Time Steps')\n",
        "            axes[idx].set_ylabel('Output Time Steps')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.save_dir}/attention_patterns.png')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_temporal_stability(self, models, city_data):\n",
        "        \"\"\"Analyze temporal stability across different horizons\"\"\"\n",
        "        horizons = [1, 6, 12, 24]\n",
        "        stability_scores = {city: [] for city in models.keys()}\n",
        "\n",
        "        for city, model_dict in models.items():\n",
        "            model = model_dict['generator']\n",
        "            data = city_data[city]['normalized']\n",
        "\n",
        "            for horizon in horizons:\n",
        "                predictions = []\n",
        "                true_values = []\n",
        "\n",
        "                # Generate predictions for different windows\n",
        "                for i in range(0, len(data)-168-horizon, 24):\n",
        "                    input_seq = data[i:i+168]\n",
        "                    with torch.no_grad():\n",
        "                        pred = model(torch.FloatTensor(input_seq).unsqueeze(0))\n",
        "                        predictions.append(pred[:, :horizon].numpy())\n",
        "                    true_values.append(data[i+168:i+168+horizon])\n",
        "\n",
        "                # Calculate stability score\n",
        "                prediction_std = np.std([p.std() for p in predictions])\n",
        "                truth_std = np.std([t.std() for t in true_values])\n",
        "                stability_scores[city].append(1 - prediction_std/truth_std)\n",
        "\n",
        "        # Plot stability scores\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        for city, scores in stability_scores.items():\n",
        "            plt.plot(horizons, scores, 'o-', label=city)\n",
        "\n",
        "        plt.title('Temporal Stability Analysis')\n",
        "        plt.xlabel('Forecast Horizon (hours)')\n",
        "        plt.ylabel('Stability Score')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(f'{self.save_dir}/temporal_stability.png')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_error_distribution(self, models, city_data):\n",
        "        \"\"\"Analyze error distributions\"\"\"\n",
        "        fig, axes = plt.subplots(3, 2, figsize=(20, 25))\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for idx, (city, model_dict) in enumerate(models.items()):\n",
        "            if idx >= len(axes):\n",
        "                break\n",
        "\n",
        "            model = model_dict['generator']\n",
        "            data = city_data[city]['normalized']\n",
        "\n",
        "            # Generate predictions for the last month\n",
        "            input_seq = data[-720:-24]  # Last month minus last day\n",
        "            true_values = data[-24:]    # Last day\n",
        "\n",
        "            with torch.no_grad():\n",
        "                predictions = model(torch.FloatTensor(input_seq).unsqueeze(0))\n",
        "                predictions = predictions.squeeze().numpy()\n",
        "\n",
        "            # Calculate errors\n",
        "            errors = predictions - true_values\n",
        "\n",
        "            # Plot error distribution\n",
        "            sns.histplot(errors, kde=True, ax=axes[idx])\n",
        "            axes[idx].set_title(f'{city} - Error Distribution')\n",
        "            axes[idx].set_xlabel('Prediction Error')\n",
        "            axes[idx].set_ylabel('Frequency')\n",
        "\n",
        "            # Add normal distribution fit\n",
        "            mu, std = stats.norm.fit(errors)\n",
        "            x = np.linspace(min(errors), max(errors), 100)\n",
        "            p = stats.norm.pdf(x, mu, std)\n",
        "            axes[idx].plot(x, p * len(errors) * (x[1]-x[0]), 'r-', lw=2,\n",
        "                         label=f'Normal: μ={mu:.2f}, σ={std:.2f}')\n",
        "            axes[idx].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.save_dir}/error_distribution.png')\n",
        "        plt.close()\n",
        "\n",
        "    def create_performance_summary(self, metrics, training_histories):\n",
        "        \"\"\"Create comprehensive performance summary\"\"\"\n",
        "        # Prepare summary data\n",
        "        summary = pd.DataFrame(metrics).T\n",
        "        summary['Convergence_Speed'] = [\n",
        "            len(hist['generator_loss']) for hist in training_histories.values()\n",
        "        ]\n",
        "        summary['Final_Training_Loss'] = [\n",
        "            hist['generator_loss'][-1] for hist in training_histories.values()\n",
        "        ]\n",
        "\n",
        "        # Plot summary metrics\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        summary.plot(kind='bar', subplots=True, layout=(3,2), figsize=(15, 20))\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.save_dir}/performance_summary.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Save summary to CSV\n",
        "        summary.to_csv(f'{self.save_dir}/performance_summary.csv')\n",
        "\n",
        "        return summary\n",
        "\n",
        "def extend_main():\n",
        "    # Get results from main analysis\n",
        "    analyzer = MultiCityAnalysis()\n",
        "\n",
        "    # Fetch raw data\n",
        "    raw_city_data = analyzer.fetch_all_city_data()\n",
        "\n",
        "    # Preprocess data\n",
        "    preprocessor = GHIDataPreprocessor()\n",
        "    city_data = preprocessor.preprocess_data(raw_city_data)\n",
        "    eda_analyzer = GHIExploratoryAnalysis()\n",
        "    print(\"\\nPerforming Exploratory Data Analysis...\")\n",
        "    dfs, summary_stats = eda_analyzer.run_complete_eda(city_data)\n",
        "\n",
        "    # Additional analysis\n",
        "    print(\"\\nCross-city Analysis:\")\n",
        "    for city in city_data.keys():\n",
        "        daily_max = dfs[city].groupby(dfs[city].index.date)['GHI'].max()\n",
        "        daily_sum = dfs[city].groupby(dfs[city].index.date)['GHI'].sum()\n",
        "        print(f\"\\n{city}:\")\n",
        "        print(f\"Average daily maximum GHI: {daily_max.mean():.2f} kW/m²\")\n",
        "        print(f\"Average daily total GHI: {daily_sum.mean():.2f} kW/m²/day\")\n",
        "    models, training_histories = analyzer.train_models(city_data)\n",
        "    training_histories = {\n",
        "        city: {\n",
        "            'generator_loss': models[city]['generator'].training_history['g_loss_avg'],\n",
        "            'discriminator_loss': models[city]['generator'].training_history['d_loss_avg']\n",
        "        } for city in models.keys()\n",
        "    }\n",
        "    metrics = analyzer.evaluate_models(models, city_data)\n",
        "\n",
        "    # Initialize advanced visualizations\n",
        "    advanced_viz = AdvancedVisualizations()\n",
        "\n",
        "    # Generate advanced visualizations\n",
        "    advanced_viz.plot_attention_patterns(models, city_data)\n",
        "    advanced_viz.plot_temporal_stability(models, city_data)\n",
        "    advanced_viz.plot_error_distribution(models, city_data)\n",
        "\n",
        "    # Create performance summary\n",
        "    summary = advanced_viz.create_performance_summary(metrics, training_histories)\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "0svYRRawTMSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "\n",
        "class ThesisExperimentManager:\n",
        "    def __init__(self, base_dir='thesis_results/'):\n",
        "        \"\"\"Initialize experiment manager with directory structure\"\"\"\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.dirs = self._create_directory_structure()\n",
        "        self.experiment_metadata = {\n",
        "            'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
        "            'cities': {},\n",
        "            'model_params': {},\n",
        "            'training_params': {},\n",
        "            'results': {}\n",
        "        }\n",
        "\n",
        "    def _create_directory_structure(self):\n",
        "        \"\"\"Create organized directory structure for thesis results\"\"\"\n",
        "        dirs = {\n",
        "            'data': self.base_dir / 'data',\n",
        "            'models': self.base_dir / 'models',\n",
        "            'plots': self.base_dir / 'plots',\n",
        "            'metrics': self.base_dir / 'metrics',\n",
        "            'attention': self.base_dir / 'attention_analysis'\n",
        "        }\n",
        "\n",
        "        for dir_path in dirs.values():\n",
        "            dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        return dirs\n",
        "\n",
        "    def save_raw_data(self, city_data):\n",
        "        \"\"\"Save raw data for each city\"\"\"\n",
        "        for city, data in city_data.items():\n",
        "            city_path = self.dirs['data'] / f'{city.lower()}'\n",
        "            city_path.mkdir(exist_ok=True)\n",
        "\n",
        "            # Save raw and normalized data\n",
        "            np.save(city_path / 'raw_data.npy', data['raw'])\n",
        "            np.save(city_path / 'normalized_data.npy', data['normalized'])\n",
        "\n",
        "            # Save metadata\n",
        "            metadata = {\n",
        "                'data_points': len(data['raw']),\n",
        "                'mean': float(np.mean(data['raw'])),\n",
        "                'std': float(np.std(data['raw'])),\n",
        "                'max': float(np.max(data['raw'])),\n",
        "                'min': float(np.min(data['raw']))\n",
        "            }\n",
        "            self.experiment_metadata['cities'][city] = metadata\n",
        "\n",
        "            with open(city_path / 'metadata.json', 'w') as f:\n",
        "                json.dump(metadata, f, indent=4)\n",
        "\n",
        "    def save_model(self, city, model_dict):\n",
        "        \"\"\"Save model and training history for each city\"\"\"\n",
        "        city_path = self.dirs['models'] / f'{city.lower()}'\n",
        "        city_path.mkdir(exist_ok=True)\n",
        "\n",
        "        # Save model state\n",
        "        torch.save(model_dict['generator'].state_dict(),\n",
        "                  city_path / 'generator.pth')\n",
        "        torch.save(model_dict['discriminator'].state_dict(),\n",
        "                  city_path / 'discriminator.pth')\n",
        "\n",
        "        # Save training history\n",
        "        history = {\n",
        "            'generator_loss': model_dict['generator'].training_history['g_loss_avg'],\n",
        "            'discriminator_loss': model_dict['generator'].training_history['d_loss_avg']\n",
        "        }\n",
        "        with open(city_path / 'training_history.json', 'w') as f:\n",
        "            json.dump(history, f, indent=4)\n",
        "\n",
        "    def save_metrics(self, metrics):\n",
        "        \"\"\"Save evaluation metrics\"\"\"\n",
        "        metrics_df = pd.DataFrame(metrics).T\n",
        "        metrics_df.to_csv(self.dirs['metrics'] / 'model_metrics.csv')\n",
        "        self.experiment_metadata['results']['metrics'] = metrics\n",
        "\n",
        "        # Create metrics visualization\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        metrics_df.plot(kind='bar')\n",
        "        plt.title('Model Performance Metrics Across Cities')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.dirs['plots'] / 'metrics_comparison.png')\n",
        "        plt.close()\n",
        "\n",
        "    def save_attention_weights(self, city, attention_weights):\n",
        "        \"\"\"Save attention weight matrices\"\"\"\n",
        "        attention_path = self.dirs['attention'] / f'{city.lower()}'\n",
        "        attention_path.mkdir(exist_ok=True)\n",
        "\n",
        "        np.save(attention_path / 'attention_weights.npy',\n",
        "                attention_weights.cpu().numpy())\n",
        "\n",
        "    def save_experiment_metadata(self):\n",
        "        \"\"\"Save complete experiment metadata\"\"\"\n",
        "        with open(self.base_dir / 'experiment_metadata.json', 'w') as f:\n",
        "            json.dump(self.experiment_metadata, f, indent=4)\n",
        "\n",
        "def run_thesis_experiment():\n",
        "    \"\"\"Main experiment execution function\"\"\"\n",
        "    # Initialize experiment manager\n",
        "    experiment = ThesisExperimentManager()\n",
        "\n",
        "    # Initialize analyzers\n",
        "    analyzer = MultiCityAnalysis()\n",
        "    preprocessor = GHIDataPreprocessor()\n",
        "    eda_analyzer = GHIExploratoryAnalysis()\n",
        "\n",
        "    # Model parameters\n",
        "    model_params = {\n",
        "        'seq_length': 168,\n",
        "        'pred_length': 24,\n",
        "        'batch_size': 32,\n",
        "        'epochs': 100,\n",
        "        'd_model': 256,\n",
        "        'num_heads': 8,\n",
        "        'num_layers': 3,\n",
        "        'dropout': 0.1\n",
        "    }\n",
        "    experiment.experiment_metadata['model_params'] = model_params\n",
        "\n",
        "    try:\n",
        "        # 1. Data Collection and Preprocessing\n",
        "        print(\"\\nFetching and preprocessing data...\")\n",
        "        raw_city_data = analyzer.fetch_all_city_data()\n",
        "        city_data = preprocessor.preprocess_data(raw_city_data)\n",
        "        experiment.save_raw_data(city_data)\n",
        "\n",
        "        # 2. Exploratory Data Analysis\n",
        "        print(\"\\nPerforming EDA...\")\n",
        "        dfs, summary_stats = eda_analyzer.run_complete_eda(city_data)\n",
        "\n",
        "        # 3. Model Training\n",
        "        print(\"\\nTraining models...\")\n",
        "        models, training_histories = analyzer.train_models(\n",
        "            city_data,\n",
        "            seq_length=model_params['seq_length'],\n",
        "            pred_length=model_params['pred_length'],\n",
        "            batch_size=model_params['batch_size'],\n",
        "            epochs=model_params['epochs']\n",
        "        )\n",
        "\n",
        "        # Save models and training histories\n",
        "        for city, model_dict in models.items():\n",
        "            experiment.save_model(city, model_dict)\n",
        "\n",
        "            # Extract and save attention weights\n",
        "            with torch.no_grad():\n",
        "                attention_weights = model_dict['generator'].get_attention_weights(\n",
        "                    torch.FloatTensor(city_data[city]['normalized'][-168:]).unsqueeze(0)\n",
        "                )\n",
        "                experiment.save_attention_weights(city, attention_weights)\n",
        "\n",
        "        # 4. Model Evaluation\n",
        "        print(\"\\nEvaluating models...\")\n",
        "        metrics = analyzer.evaluate_models(models, city_data)\n",
        "        experiment.save_metrics(metrics)\n",
        "\n",
        "        # 5. Generate Advanced Visualizations\n",
        "        print(\"\\nGenerating visualizations...\")\n",
        "        advanced_viz = AdvancedVisualizations()\n",
        "        advanced_viz.plot_attention_patterns(models, city_data)\n",
        "        advanced_viz.plot_temporal_stability(models, city_data)\n",
        "        advanced_viz.plot_error_distribution(models, city_data)\n",
        "\n",
        "        # 6. Save Final Experiment Metadata\n",
        "        experiment.save_experiment_metadata()\n",
        "\n",
        "        print(\"\\nExperiment completed successfully!\")\n",
        "        return experiment.experiment_metadata\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during experiment: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    metadata = run_thesis_experiment()\n",
        "    print(\"\\nExperiment Summary:\")\n",
        "    print(json.dumps(metadata, indent=2))"
      ],
      "metadata": {
        "id": "J_CVNhlNWgGl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "87754aa8-436d-4eb1-ac0e-cd07074720b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching and preprocessing data...\n",
            "Fetching data for Delhi...\n",
            "Fetching data for Bangalore...\n",
            "\n",
            "Performing data preprocessing...\n",
            "\n",
            "Processing Delhi data:\n",
            "Found 2319 sudden changes in GHI\n",
            "\n",
            "Preprocessing summary for Delhi:\n",
            "Total data points: 8760\n",
            "Final missing values: 0\n",
            "Data range: 0.00 to 1021.65\n",
            "Mean GHI: 196.77\n",
            "Std GHI: 265.71\n",
            "\n",
            "Processing Bangalore data:\n",
            "Found 2263 sudden changes in GHI\n",
            "\n",
            "Preprocessing summary for Bangalore:\n",
            "Total data points: 8760\n",
            "Final missing values: 0\n",
            "Data range: 0.00 to 1078.40\n",
            "Mean GHI: 231.03\n",
            "Std GHI: 300.55\n",
            "\n",
            "Performing EDA...\n",
            "\n",
            "Performing Exploratory Data Analysis...\n",
            "\n",
            "Validating data for Delhi:\n",
            "Number of raw data points: 8760\n",
            "Number of normalized data points: 8760\n",
            "\n",
            "Validating data for Bangalore:\n",
            "Number of raw data points: 8760\n",
            "Number of normalized data points: 8760\n",
            "\n",
            "Summary Statistics for Each City:\n",
            "\n",
            "Delhi:\n",
            "count    8760.000000\n",
            "mean      196.772393\n",
            "std       265.707036\n",
            "min         0.000000\n",
            "25%         0.000000\n",
            "50%        34.071667\n",
            "75%       362.407500\n",
            "max      1021.650000\n",
            "Name: GHI, dtype: float64\n",
            "\n",
            "Daily max average: 679.52\n",
            "Daily duration of non-zero GHI: 13.4 hours\n",
            "\n",
            "Bangalore:\n",
            "count    8760.000000\n",
            "mean      231.028187\n",
            "std       300.548441\n",
            "min         0.000000\n",
            "25%         0.000000\n",
            "50%        37.058333\n",
            "75%       455.737500\n",
            "max      1078.400000\n",
            "Name: GHI, dtype: float64\n",
            "\n",
            "Daily max average: 803.13\n",
            "Daily duration of non-zero GHI: 13.5 hours\n",
            "\n",
            "Training models...\n",
            "\n",
            "Training model for Delhi...\n",
            "Training on device: cuda\n",
            "Epoch [10/100], G Loss: 0.0703, D Loss: 1.3878\n",
            "Epoch [20/100], G Loss: 0.0697, D Loss: 1.3875\n",
            "Epoch [30/100], G Loss: 0.0694, D Loss: 1.3865\n",
            "Epoch [40/100], G Loss: 0.0695, D Loss: 1.3864\n",
            "Epoch [50/100], G Loss: 0.0695, D Loss: 1.3862\n",
            "Epoch [60/100], G Loss: 0.0694, D Loss: 1.3866\n",
            "Epoch [70/100], G Loss: 0.0694, D Loss: 1.3865\n",
            "Epoch [80/100], G Loss: 0.0694, D Loss: 1.3861\n",
            "Epoch [90/100], G Loss: 0.0693, D Loss: 1.3864\n",
            "Epoch [100/100], G Loss: 0.0694, D Loss: 1.3860\n",
            "\n",
            "Training model for Bangalore...\n",
            "Training on device: cuda\n",
            "Epoch [10/100], G Loss: 0.0702, D Loss: 1.3872\n",
            "Epoch [20/100], G Loss: 0.0698, D Loss: 1.3875\n",
            "Epoch [30/100], G Loss: 0.0695, D Loss: 1.3870\n",
            "Epoch [40/100], G Loss: 0.0695, D Loss: 1.3867\n",
            "Epoch [50/100], G Loss: 0.0694, D Loss: 1.3869\n",
            "Epoch [60/100], G Loss: 0.0694, D Loss: 1.3865\n",
            "Epoch [70/100], G Loss: 0.0694, D Loss: 1.3867\n",
            "Epoch [80/100], G Loss: 0.0693, D Loss: 1.3862\n",
            "Epoch [90/100], G Loss: 0.0693, D Loss: 1.3864\n",
            "Epoch [100/100], G Loss: 0.0694, D Loss: 1.3864\n",
            "\n",
            "Error during experiment: 'AST' object has no attribute 'get_attention_weights'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'AST' object has no attribute 'get_attention_weights'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6d186f81c818>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_thesis_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nExperiment Summary:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-6d186f81c818>\u001b[0m in \u001b[0;36mrun_thesis_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;31m# Extract and save attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 attention_weights = model_dict['generator'].get_attention_weights(\n\u001b[0m\u001b[1;32m    159\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'normalized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m168\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1932\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'AST' object has no attribute 'get_attention_weights'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zAP03RdVdkVq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}